Artificial neural networks are biologically inspired classification algorithms that consist of an input layer of nodes, one or more hidden layers and an output layer. 
Each node in a layer has one corresponding node in the next layer, thus creating the
stacking eﬀect [1].
Artificial neural networks are the very versatile tools and have been widely used to tackle many issues [2-6]. 
Feed-forward neural networks (FNN) are one of the popular structures among artificial neural networks. 
These eﬃcient networks are widely used to solve complex problems by modeling complex input-output relationships.
However, FNNs often end up being over trained. 
They adopt trials-and-errors to seek possible values of parameters for convergence of the global optimum. 
The learning process of an FNN cannot guarantee the global optimum, sometimes trapping the network into the local optimum. The back-propagation learning algorithm (BPLA) is a widely used method for FNN learning in many applications. 
It has the great advantage of simple implementation [9].
In addition, many studies have indicated that genetic algorithms (GA) can be successfully applied to identity global optimizations of multidimensional
functions [10,11]. 
GAs are widely applied in the optimization of the parameters spaces of neural networks. Traditional optimization techniques can also determine the number of network parameters, such as network connection weightings; however, they are not able to control the parameter optimizations in the absence of gradient information.
In contrast, GAs are able to resolve this issue. 
They can be widely used in all aspects of neural networks. Examples include the determination of network structures (the number of input nodes, the number of hidden layers, the number of nodes in the hidden layers, and the number of output nodes), and the optimization of parameters such as the selection of node switch functions, connection weightings and learning speeds [12].
Sexton and Gupta [13] pointed out that GAs are able to process a large variety of data
types, as soon as they can be expressed with bits of fixed lengths. GAs also yield good
outcomes for optimization. 
The above study proved that GAs are superior to BPLAs.
However, their study does not specify the sources and quantities of their data. In other
words, they do not use diﬀerent data types to train and compare a GA and BPLA.
Based on the above analysis, this study focuses on performances in the training of FNN
with BPLAs and GAs.
The data types used to compare the performances of these two
algorithms are Sin function, Iris plants and Diabetes.Section 2 introduces the concepts of BPLA and GA; Section 3 expresses the procedures of BPLA and GA for FNN training; Sections 4 and 5 describe the data types, results and analyses; finally, conclusions are given in Section 6. 
Back-propagation neural network. 
BPLAs were proposed by Rumelhart et al. [14]. 
They have since become famous learning algorithms among ANNs. 
In the learning process, to reduce the inaccuracy of ANNs, BPLAs use the gradient-decent search method to adjust the connection weights.
The output of each neuron is the aggregation of the numbers of neurons of the
previous level multiplied by its corresponding weights. 
The input values are converted into output signals with the calculations of activation functions [15]. 
Back-propagation ANNs have been widely and successfully applied in diverse applications, such as pattern recognition, location selection and performance evaluations.
Wang and Bai [16] applied a back-propagation ANN to assess and predict the quality of
water by overcoming problems associated with heavy workloads and excessive subjectivity
in traditional assessment methods. 
The results were objective and accurate.
There is no need to carry out complex pre-processing in order to monitor data and the workload is light.
Wu et al. [17] established a back-propagation ANN for the identification of lifespan distributions of for machinery products. 
The simulation results show that backpropagation ANNs are suitable for identifying the lifespan distribution models when large ranges of parameters exist.
Bongards [18] used back-propagation to predict and control the improvement in density of ammonia and total nitrogen in waste water processing plants. 
Chou et al. [3] and Xiao and Chandrasekar [19] also successfully applied back-propagation ANNs for patterning of e-commerce customers patterning and rainfall estimation from radar data separately. 
Che [20] employed a back-propagation ANN for product and mold cost estimation of plastic injection molding. 2.2. Genetic algorithms. GAs were proposed by Holland in 1975, as optimization search mechanisms for natural selection. The fundamental principle is to imitate the law of natural selection in nature by choosing parents with better characteristics. 
Random interactions of bit information are carried out, in the hope of generating oﬀspring superior to parents. 
The continuous repeats see the birth of optimal species with the strongest adaptability. Chang et al. [21] applied a GA and the testing design principles to develop the optimal design model for underground wells.
The outcome shows that the monitoring well should be located in the area adjacent to the water pumping well and the water pumping well should be deployed in the region with smaller transmission.
This reduces the uncertainties of estimation parameters.
Jiang and Yuan [22] proposed credit valuations based on GAs and neural networks, and controlled the occurrence of Type-2 errors of commercial banks that lead to heavy losses in the design of credit valuations by using GA adaptability functions. 
The results show that the model is high in reliability and applicability.
Jiang and Zhai [23] adopted neural networks and a GA to design thinking patterns of NPC roles in order to establish a self-learning model that is more autonomous and intelligent than the traditional NPC roles. 
In addition, GAs have been used to solve complicated problems effectively. 
For example, refer to Wu et al. [24], Che [25-27], Che and Wang [28], Chang [29], Sha and Che [30], Che and Wang [31] and Che and Chaing [32].BPLA for FNN training.
The BPLA structure is based mainly on batch learning. 
Through iterations and modifications, the average square errors of test data are used to determine the optimal weights and biases.
CPU time to run the BPLA is shorter than that to run the GA. 
In terms of MSE, the BPLA reports fewer errors than the GA.
This study focuses on training a FNN by using a BPLA and GA. 
To compare the performances of the BPLA and GA, three data sets, Sin function, Iris plants and Diabetes, are employed in this study.
According to the measurement indicators, the BPLA is indeed slightly superior to the GA under certain conditions.
Also, the BPLA is faster than the GA in terms of training speeds.
However, the BPLA has the problem of overtraining, whereas, the GA does not.
In terms of CPU time required, the BPLA yields better results than the GA. 
As far as the influence of diﬀerent learning rates and momentum coeﬃcients on BPLA convergence, the experiments show that learning rates influence the speed of convergence.
Also, when the momentum coeﬃcients are small, they also aﬀect the convergence speed of MSE. When the learning rate is high and the momentum coeﬃcient is large, MSE will overly fluctuate.
However, when both parameters are not too high, the MSE convergence does not fluatuate. Neural networks have learning and fault-tolerance characteristics.
GAs have the capabilities to find the best solutions in global searches.
This study finds that in the process of seeking solutions, back-propagation ANNs report more stable convergence of best solutions, but require a larger number of learning cycles. 
On the other hand, GAs require a smaller number of learning cycles to find the best solutions, but need longer training time. 
To enhance the FNN training.